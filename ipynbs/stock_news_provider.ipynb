{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T00:17:24.043900Z",
     "start_time": "2025-10-18T00:17:24.007721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize an Ollama Client for our generative Llm model\n",
    "\n",
    "text_model = \"llama3.2\"\n",
    "\n",
    "def llm_client_loader():\n",
    "    \"\"\"This function serves an Ollama-hosted text generator model, to be used by our graphs.\"\"\"\n",
    "    try:\n",
    "        llm = ChatOllama(\n",
    "            model=text_model,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e} instantiating the Ollama client, is the Ollama server running?.\")\n",
    "\n",
    "llm = llm_client_loader()"
   ],
   "id": "b5bfb586f07bdd70",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T00:17:34.292251Z",
     "start_time": "2025-10-18T00:17:34.287075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
    "\n",
    "tools = [YahooFinanceNewsTool()]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ],
   "id": "bbd9836d4070ec1e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5c4ea25ff59b58aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain.messages import SystemMessage, HumanMessage, ToolMessage\n",
    "import os\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import END\n",
    "from pydantic import Field, BaseModel\n",
    "from langgraph.graph import MessagesState\n",
    "from typing import Annotated, Literal\n",
    "import operator\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.types import Send\n",
    "from typing import List, TypedDict\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call(state: MessagesState):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "# Nodes\n",
    "def llm_call(state: MessagesState):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END]\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "# Show the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = agent.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ],
   "id": "b2c400ee472e971c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import END\n",
    "from pydantic import Field, BaseModel\n",
    "from langgraph.graph import MessagesState\n",
    "from typing import Annotated, Literal\n",
    "import operator\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.types import Send\n",
    "from typing import List, TypedDict\n",
    "\n",
    "# AAA\n",
    "try:\n",
    "    # Schema for structured output\n",
    "    #State definitions...\n",
    "    class News_Analyzer(BaseModel):\n",
    "        news_article_content: str = Field(description=\"The news article content to be analyzed.\")\n",
    "        metadata: str = Field(description=\"The metadata for the specific news article.\")\n",
    "\n",
    "    # Internal states (dynamic)\n",
    "    # Augment the LLM with schema for structured output\n",
    "    planner = llm.with_structured_output(FinancialReport)\n",
    "\n",
    "    # Internal State definition\n",
    "\n",
    "    class State(MessagesState):\n",
    "        news_source: str  # news articles all together\n",
    "        news_articles: list[News_Analyzer]  # list of news articles divided into metadata and content\n",
    "        completed_analyses: Annotated[\n",
    "            list, operator.add\n",
    "        ]  # Shared key for the analysts to write to\n",
    "        macro_financial_report: str\n",
    "\n",
    "    class WorkerState(TypedDict):\n",
    "        news_article: News_Analyzer\n",
    "        completed_analyses: Annotated[list, operator.add]  # keys must match with other State!\n",
    "\n",
    "    # Nodes / Tools\n",
    "    def orchestrator(state: State):\n",
    "        try:\n",
    "            \"\"\"Orchestrator that creates a research plan in order to obtain a comprehensive  ona set of topics it has to conduct research on,\n",
    "             metadata and content.\n",
    "            \"\"\"\n",
    "\n",
    "            # Generate queries...\n",
    "            news_articles = planner.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"Split the following text into into distinct singular news articles, each with corresponding metadata and content.\"\n",
    "                                \"Do not mix news articles together, keep each topic in ints own container/bucket.\"),\n",
    "                    HumanMessage(content=f\"Assign to each split both the content and corresponding metadata: {state[\"news_source\"]}.\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            return {\"news_articles\": news_articles.news_analysts}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error {e} during the orchestrator process.\")\n",
    "\n",
    "    class News_Summary(BaseModel):\n",
    "        summary: str = Field(description=\"The summary of the news article.\")\n",
    "        label: Literal[\"inflation\", \"rates\", \"fed\", \"macro\"] = Field(description=\"The label for the news article.\")\n",
    "\n",
    "    summarizer = llm.with_structured_output(News_Summary)\n",
    "\n",
    "    def llm_call(state: WorkerState):\n",
    "        \"\"\"Worker writes a summary about the given news article. It also classifies the news article with a\n",
    "        macroeconomic label.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            print(f\"LLM call instantiated {state['news_article'].metadata}.\")\n",
    "\n",
    "            analysis = summarizer.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"Summarize the following news article in two clear and concise paragraph, capturing the key ideas \"\n",
    "                                \"without missing critical points. Ensure the summary is easy to understand and avoids \"\n",
    "                                \"excessive detail. Be sure to also label the news article accordingly.\"\n",
    "                    ),\n",
    "                    HumanMessage(\n",
    "                        content=f\"Here is the news article: {state['news_article'].news_article_content} with corresponding metadata: \"\n",
    "                                f\"{state['news_article'].metadata}.\"\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            serialized_summary = \"\".join(f\"{analysis.summary} {analysis.label}\")\n",
    "\n",
    "            print(f\"Summary is {serialized_summary}.\")\n",
    "\n",
    "            # Write the final amount that was calculated.\n",
    "            return {\"completed_analyses\": [serialized_summary]}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error {e} during the llm-call process.\")\n",
    "\n",
    "    def synthesizer(state: State):\n",
    "        \"\"\"Synthesize a summary report from the collection of news articles. Don't forget to include the\n",
    "        macroeconomic label for each news article.\"\"\"\n",
    "        try:\n",
    "            # List of completed sections\n",
    "            completed_report = state[\"completed_analyses\"]\n",
    "\n",
    "            # Format completed section to str to use as context for final sections\n",
    "            serialized_completed_report = \"\\n\\n---\\n\\n\".join(completed_report)\n",
    "\n",
    "            return {\"macro_financial_report\": serialized_completed_report}\n",
    "        except Exception as e:\n",
    "            print(f\"Error {e} during the synthesizer process.\")\n",
    "\n",
    "    # Conditional edge function to create llm_call workers that each write a section of the report\n",
    "    def assign_workers(state: State):\n",
    "        try:\n",
    "            \"\"\"Assign a worker to each news article in the collection of news articles.\"\"\"\n",
    "\n",
    "            # Kick off section writing in parallel via Send() API\n",
    "            return [Send(\"llm_call\", {\"news_article\": s}) for s in state[\"news_articles\"]]\n",
    "        except Exception as e:\n",
    "            print(f\"Error {e} during the worker assignment process.\")\n",
    "\n",
    "    # Build workflow\n",
    "    orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "    # Add the nodes\n",
    "    orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "    orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "    orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "    # Add edges to connect nodes\n",
    "    orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "    orchestrator_worker_builder.add_conditional_edges(\n",
    "        \"orchestrator\", assign_workers, [\"llm_call\"]\n",
    "    )\n",
    "    orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "    orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "    # Compile the workflow\n",
    "    orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "    return orchestrator_worker\n",
    "except Exception as e:\n",
    "    print(f\"Error {e} during the graph building process.\")\n",
    "\n",
    "# Invoke\n",
    "llm_client = llm_client_loader()\n",
    "graph = graph_builder(llm_client)\n",
    "render_graph(graph)\n",
    "mn = MarketNewsProvider(category=\"forex\", api_key=os.getenv(\"FINNHUB_API_KEY\"))\n",
    "market_news = mn.fetch()\n",
    "state = graph.invoke({\"news_source\": f\"{market_news}\"})\n",
    "print(state[\"macro_financial_report\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
